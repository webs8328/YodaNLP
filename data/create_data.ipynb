{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (2.12.0)\n",
      "Requirement already satisfied: transformers in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (4.32.1)\n",
      "Requirement already satisfied: huggingface_hub in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: packaging in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: filelock in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (4.7.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: six in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/nathanielalexis/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers huggingface_hub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generics_kb (/Users/nathanielalexis/.cache/huggingface/datasets/generics_kb/generics_kb/1.0.0/9b41cde494db24f842a9260588bcfb2e3a257364568666ef240e98c70fb0e709)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c872079098924a93a1e8cd29a06859cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb = load_dataset(\"generics_kb\", name = 'generics_kb')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to generate Yoda Text from https://github.com/yevbar/Yoda-Script/blob/master/yoda.py\n",
    "import spacy\n",
    "punctuation = [',', '.', ';', '?']\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "comma = nlp('Hello, World')[1]\n",
    "\n",
    "def sentify(text):\n",
    "    output = []\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        sentence = []\n",
    "        end_punctuation = sent[-1]\n",
    "        for clause in clausify(sent[:-1]):\n",
    "            sentence.append(yodafy(clause))\n",
    "        sentence[-1].append(end_punctuation)\n",
    "        output.append(sentence)\n",
    "    return output\n",
    "\n",
    "def clausify(sent):\n",
    "    output = []\n",
    "    cur = []\n",
    "\n",
    "    prev_token = None\n",
    "    for token in sent:\n",
    "        if prev_token!= None and prev_token.dep_ == 'punct' and prev_token.text in punctuation and token.dep_ != \"amod\":\n",
    "            #(token.dep_ == 'cc' or (token.dep_ == 'punct' and token.text in punctuation)):\n",
    "            output.append(cur)\n",
    "            #output.append([token])\n",
    "            cur = [token]\n",
    "        else:\n",
    "            cur.append(token)\n",
    "        prev_token = token\n",
    "    if cur != []:\n",
    "        output.append(cur)\n",
    "    return output\n",
    "\n",
    "def yodafy(clause):\n",
    "    new_array = []\n",
    "    state = False\n",
    "    prev_token = None\n",
    "    flag = False\n",
    "    for token in clause:\n",
    "        if state:\n",
    "            new_array.append(token)\n",
    "        elif not state and prev_token != None and (prev_token.dep_ == \"ROOT\" or prev_token.dep_ == \"aux\") and token.dep_ != \"nsubj\" or flag:\n",
    "            state = True\n",
    "            new_array.append(token)\n",
    "        elif not state and prev_token != None and (prev_token.dep_ == \"ROOT\" or prev_token.dep_ == \"aux\"):\n",
    "            flag = True\n",
    "        prev_token = token\n",
    "    if len(new_array) > 0 and new_array[len(new_array)-1].dep_ != 'punct':\n",
    "        new_array.append(comma)\n",
    "    prev_token = None\n",
    "    for token in clause:\n",
    "        new_array.append(token)\n",
    "        if (token.dep_ == \"ROOT\" or token.dep_ == \"aux\") and flag == False:\n",
    "            break\n",
    "        elif prev_token != None and (prev_token.dep_ == \"ROOT\" or prev_token.dep_ == \"aux\") and flag:\n",
    "            break\n",
    "        prev_token = token\n",
    "    return new_array\n",
    "\n",
    "def yoda(string_):\n",
    "    string = []\n",
    "    #end_punctuation = string_[-1]\n",
    "    yodafied = sentify(string_)\n",
    "    for sentence in yodafied:\n",
    "        sentence_ = \"\"\n",
    "        for clause in sentence:\n",
    "            for token in clause:\n",
    "                if token.dep_ == 'NNP' or token.dep_ == 'NNPS' or token.text == 'I':\n",
    "                    sentence_ += token.text + \" \"\n",
    "                elif sentence_ == \"\" and token.dep_ == 'neg':\n",
    "                    sentence_ += \"Not\" + \" \"\n",
    "                elif sentence_ == \"\":\n",
    "                    sentence_ += token.text[0].upper() + token.text[1:] + \" \"\n",
    "                elif token.dep_ == 'punct':\n",
    "                    sentence_ = sentence_[:len(sentence_)-1] + token.text + \" \"\n",
    "                else:\n",
    "                    sentence_+=token.text.lower() + \" \"\n",
    "        string.append(sentence_)\n",
    "    return \"\".join(string) #+ end_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The settings if the power ever goes off, aa batteries maintain. '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yoda(\"AA batteries maintain the settings if the power ever goes off.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional support is provided through merchandising, group tours and affinity programs.\n",
      "dict_keys(['source', 'term', 'quantifier_frequency', 'quantifier_number', 'generic_sentence', 'score'])\n"
     ]
    }
   ],
   "source": [
    "print(imdb['train'][38994]['generic_sentence'])\n",
    "print(imdb['train'][38994].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3433000\n"
     ]
    }
   ],
   "source": [
    "print(len(imdb['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "indices = np.random.randint(0, len(imdb['train'])-1, size=50000)\n",
    "for i in indices:\n",
    "    arr.append(imdb['train'][int(i)]['generic_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Degree requirements Thirty-two credits are required for the degree.', 'Issue permits for plants which break and can eggs.', 'Earlier submission is encouraged and appreciated.', 'War simply increases the hardship on children and families struggling to gain basic foods.', 'Participants can walk or run as individuals or in teams.', 'Most people pride themselves on their ability to fit in with others.', 'All systems fall on a continuum between the two.', 'Posts there get to few readers.', 'Greens are fast and true.', 'Students design their flexible clusters of expertise in consultation with their tutors.']\n"
     ]
    }
   ],
   "source": [
    "print(arr[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "print(\".\\n.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_funcs' from '/home/webs8328/cs182_proj/data/data_funcs.py'>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import data_funcs as df\n",
    "import importlib\n",
    "importlib.reload(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = df.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source', 'term', 'quantifier_frequency', 'quantifier_number', 'generic_sentence', 'score'],\n",
      "    num_rows: 3433000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Tell me about a.net income.\n",
      "Yoda: By net sales, a.net income is divided. \n",
      "User: Tell me about aa battery.\n",
      "Yoda: The settings if the power ever goes off, aa batteries maintain. \n",
      "User: Tell me about aardvark.\n",
      "Yoda: An amount of short coarse hair, aardvarks only have. \n"
     ]
    }
   ],
   "source": [
    "x = df.format_data_yoda(h, [1, 9, 200])\n",
    "for i in x:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
